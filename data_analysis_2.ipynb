{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the library used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysubgroup as ps\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "import ast\n",
    "import string \n",
    "import imblearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_preprocess(tags):\n",
    "    \"\"\"\n",
    "    input: tag string\n",
    "    output: list of individual tags in the given tag string\n",
    "    function: preprocess a single tag string \n",
    "    \"\"\"\n",
    "    #tags = tags.replace(\"'\",\"\")\n",
    "    #tags = tags.replace(\" \",\"\")\n",
    "    #tags = tags.replace(\"[\",\"\")\n",
    "    #tags = tags.replace(\"]\",\"\")\n",
    "    tags = str(tags)\n",
    "    tags = tags.split(\" \")\n",
    "    tags = [x.lower() for x in tags]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredients_preprocess(df):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: list of distinct ingredient list\n",
    "    function: preprocess the ingredient columns and return a list of distinct ingredients\n",
    "    \"\"\"\n",
    "    distinct_ingredients = []\n",
    "    dataframe = df\n",
    "    \n",
    "    for i in range(len(dataframe)):\n",
    "        ingredients = dataframe.iloc[i]['ingredient']\n",
    "        \n",
    "        r = re.compile('[A-Z]{1}[a-zA-Z]+')\n",
    "        ingredients = str(ingredients)\n",
    "        #ingredients = ''.join(i for i in ingredients if not i.isdigit())\n",
    "        ingredients = ingredients.replace(\"'\",\"\")\n",
    "        #ingredients = ingredients.replace(\" \",\"\")\n",
    "        ingredients = ingredients.replace(\"[\",\"\")\n",
    "        ingredients = ingredients.replace(\"]\",\"\")\n",
    "        # remove text inside parentheses\n",
    "        ingredients = re.sub(r'\\([^())]*\\)',\"\", ingredients)\n",
    "        ingredients = ingredients.split(\" \")\n",
    "        ingredients = list(filter(r.match, ingredients))\n",
    "        ingredients = [x.lower() for x in ingredients]\n",
    "        distinct_ingredients += ingredients\n",
    "        dataframe.at[i, 'ingredient'] = ingredients\n",
    "        #dataframe.set_value(i, 'ingredient', ingredients)\n",
    "        \n",
    "    return [list(set(distinct_ingredients)),dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_countries(countries, data):\n",
    "    \"\"\"\n",
    "    input: list of countries, dataframe\n",
    "    output: selected dataframe whose recipes is from these countries\n",
    "    function: select the rows in dataframe whose \"tag\" value contain one country tag\n",
    "    \"\"\"\n",
    "    # add a new column class \n",
    "    drop_index = []\n",
    "    for i in range(len(data)):\n",
    "        tags = data.loc[i][\"tags\"]\n",
    "        tags = tags_preprocess(tags)\n",
    "        \n",
    "        country_same =[l for l in countries if l in tags]\n",
    "            \n",
    "        if len(country_same) == 1:\n",
    "            data.at[i, 'label'] = country_same[0]\n",
    "        if len(country_same) == 0:\n",
    "            drop_index.append(i)\n",
    "        if len(country_same) > 1:\n",
    "            drop_index.append(i)\n",
    "            #data.at[i, 'label'] = 'overlap'\n",
    "            \n",
    "    # drop the columns which has no season tags\n",
    "    data = data.drop(data.index[drop_index])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(arr):\n",
    "    \"\"\"\n",
    "    Helper function to convect an array of ingredients to a dictionary\n",
    "    \"\"\"\n",
    "    d={}\n",
    "    for a in arr:\n",
    "        d[a]=1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract comment user dataset from original dataset\n",
    "def extract_com_user(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with all the comment user information\n",
    "    function: spilt the dictionary of the column 'comment user' in original dataset\n",
    "    \"\"\"\n",
    "    df_com = pd.DataFrame()\n",
    "    for index, item in data['comment_user'].iteritems():\n",
    "        if (item != '[]'):\n",
    "            if (item != 'no comment'):\n",
    "                array = ast.literal_eval(item)\n",
    "                df_array = pd.DataFrame(array)\n",
    "                df_array['recipe_id'] = index\n",
    "                df_com = pd.concat([df_com,df_array])\n",
    "    return df_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_cat_in_com(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with multi colunms\n",
    "    function: add one subcategory of recipes to comment user dataset \n",
    "    \"\"\"    \n",
    "    punct = set(string.punctuation) \n",
    "    list_sub_cat = []\n",
    "    \n",
    "    df_com2 = pd.DataFrame()\n",
    "    for index, item in data['calorie_value'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_sub = list(item)\n",
    "            list_sub = ''.join(x for x in list_sub if x not in punct)\n",
    "            list_sub_cat.append(list_sub)\n",
    "    df_sub_cat = pd.DataFrame(list_sub_cat)\n",
    "    df_sub_cat['calorie_value'] = df_sub_cat\n",
    "    \n",
    "    df_sub_cat['recipe_id'] = data['calorie_value'].index     \n",
    "    df_com2 = pd.concat([df_com2,df_sub_cat])\n",
    "    return df_com2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recipe_info(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with multi colunms\n",
    "    function: add one subcategory of recipes to comment user dataset \n",
    "    \"\"\"    \n",
    "    punct = set(string.punctuation) \n",
    "    # add recipe name in comment users data\n",
    "    list_recipe_name = []\n",
    "    df_recipe = pd.DataFrame()\n",
    "    for index, item in data['recipe_name'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_name = list(item)\n",
    "            list_name = ''.join(x for x in list_name if x not in punct)\n",
    "            list_recipe_name.append(list_name)\n",
    "\n",
    "    df_name = pd.DataFrame(list_recipe_name)\n",
    "    df_name['recipe_id'] = data['recipe_name'].index \n",
    "    df_name = df_name.set_index([\"recipe_id\"])\n",
    "    df_name\n",
    "    df_recipe['recipe_name'] = df_name[0]\n",
    "\n",
    "    # add recipe difficulty in comment users data\n",
    "    list_recipe_diff = []\n",
    "    df_recipe_diff = pd.DataFrame()\n",
    "    for index, item in data['difficulty'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_diff = list(item)      \n",
    "            list_diff = ''.join(x for x in list_diff if x not in punct)   \n",
    "            list_recipe_diff.append(list_diff)\n",
    "\n",
    "    df_diff = pd.DataFrame(list_recipe_diff)\n",
    "    df_diff['recipe_id'] = data['difficulty'].index \n",
    "    df_diff = df_diff.set_index([\"recipe_id\"])\n",
    "    df_recipe['difficulty'] = df_diff[0]\n",
    "\n",
    "    # add recipe preparation_time in comment users data\n",
    "    list_recipe_pre = []\n",
    "    df_recipe_pre = pd.DataFrame()\n",
    "    for index, item in data['preparation_time'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_pre = list(item)\n",
    "\n",
    "            list_pre = ''.join(x for x in list_pre if x not in punct)\n",
    "\n",
    "            list_recipe_pre.append(list_pre)\n",
    "\n",
    "    df_pre = pd.DataFrame(list_recipe_pre)\n",
    "    df_pre['recipe_id'] = data['preparation_time'].index \n",
    "    df_pre = df_pre.set_index([\"recipe_id\"])\n",
    "    df_recipe['preparation_time'] = df_pre[0]\n",
    "\n",
    "    # add recipe tags in comment users data\n",
    "    list_recipe_tags = []\n",
    "    df_recipe_tags = pd.DataFrame()\n",
    "    for index, item in data['tags'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_tags = list(item)      \n",
    "            list_tags = ''.join(x for x in list_tags if x not in punct)   \n",
    "            list_recipe_tags.append(list_tags)\n",
    "\n",
    "\n",
    "    df_tags = pd.DataFrame(list_recipe_tags)\n",
    "    df_tags['recipe_id'] = data['tags'].index \n",
    "    df_tags = df_tags.set_index([\"recipe_id\"])\n",
    "    df_recipe['tags'] = df_tags[0]\n",
    "\n",
    "    # add recipe ingredient in comment users data\n",
    "    list_recipe_ingredient = []\n",
    "    df_recipe_ingredient = pd.DataFrame()\n",
    "    for index, item in data['ingredient'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_ingredient = list(item) \n",
    "            list_ingredient = ''.join(x for x in list_ingredient if x not in punct)\n",
    "            list_recipe_ingredient.append(list_ingredient)\n",
    "\n",
    "    df_recipe_ingredient = pd.DataFrame(list_recipe_ingredient)\n",
    "    df_recipe_ingredient['recipe_id'] = data['ingredient'].index \n",
    "    df_recipe_ingredient = df_recipe_ingredient.set_index([\"recipe_id\"])\n",
    "    df_recipe['ingredient'] = df_recipe_ingredient[0]\n",
    "    return df_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_group(age):   \n",
    "    \"\"\"\n",
    "    input: age value\n",
    "    output: group description\n",
    "    function: divide age value uinto 5 groups \n",
    "    \"\"\"   \n",
    "    bucket = None\n",
    "    age = int(age)    \n",
    "    if age < 30:\n",
    "        bucket = '<30 Jahre'    \n",
    "    if age in range(30, 60):\n",
    "        bucket = '30-60 Jahre'        \n",
    "    if age >= 61:\n",
    "        bucket = '60+ Jahre'\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calorie_level(calorie):   \n",
    "    \"\"\"\n",
    "    input: calorie value\n",
    "    output: group description\n",
    "    function: divide calorie value into 3 groups \n",
    "    \"\"\"   \n",
    "    bucket = None\n",
    "    calorie = int(calorie)    \n",
    "    if calorie < 300:\n",
    "        bucket = 'low_calorie'    \n",
    "    if calorie in range(300, 500):\n",
    "        bucket = 'medium_calorie'        \n",
    "    if calorie >= 500:\n",
    "        bucket = 'high_calorie'      \n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_None(data, name):\n",
    "    \"\"\"\n",
    "    Helper function to remove None value in one column\n",
    "    \"\"\" \n",
    "    y = data[data[name] == 'None']\n",
    "    index_n = y.index.tolist()\n",
    "    data = data.drop(index = index_n)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(data, calorie_level):\n",
    "    df_sub_group[country] = df_dum_car[calorie_level]\n",
    "    return df_sub_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_time_group(pre_time):   \n",
    "    pre_time = int(pre_time)    \n",
    "    if pre_time < 30:\n",
    "        bucket = '<30 Min'    \n",
    "    if pre_time in range(30, 61):\n",
    "        bucket = '30-60 Min'                \n",
    "    if pre_time in range(60, 91):\n",
    "        bucket = '60-90 Min'\n",
    "    if pre_time >= 91:\n",
    "        bucket = '90+ Min'\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgroup Discovery\n",
    "\n",
    "In this section we will use subgroup discovery to explore the association rules between attributes\n",
    "\n",
    "- why we choose subgroup discovery?\n",
    "\n",
    "because we find out that subgroup discovery is quite powerful compared to other data mining techniques. As long as we set differnt target with different search space, we can use use subgroup discovery technique to dig almost all interesting pattern that we want to explore from the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the recipe id from recipe urls\n",
    "list_cat_no = []\n",
    "i = 0\n",
    "for item in data['recipe_url']:\n",
    "    list_cat_no.append(item.split('/')[4])\n",
    "    \n",
    "# add one column \"recipe_id\" into the dataset and set it as the index of dataset\n",
    "data['recipe_id'] = list_cat_no\n",
    "data = data.set_index([\"recipe_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules between comment user information and recipe attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data imbalance: gender\n",
    "- improve the quality of data (preprossing)\n",
    "- sampling \n",
    "- improne quality of result: different search algorithmn and selectors choices\n",
    "- if the quality is also very low, plot the t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- associations with \"calorie\" and comment user information\n",
    "\n",
    "since the job has more than half non values, so here we only explore the the association between  marriage_status, gender, and age with recipe calorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19058"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the numerical value string in the column calorie \n",
    "pat = r\"([-+]?\\d*\\.\\d+|\\d+)\"\n",
    "data[\"calorie_value\"] = data[\"calorie\"].str.extract(pat, flags=0, expand=True)\n",
    "\n",
    "# drop all rows with nan value in both columns comment_user and calorie_value\n",
    "data_com = data.dropna(subset=[\"comment_user\",'calorie_value'])\n",
    "len(data_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the corresponding recipe data for each comment_user\n",
    "#data_com = data_com[0:500]\n",
    "# data_ingrent = ingredients_preprocess(data_com)\n",
    "# data_ingrent\n",
    "df_com_1 = extract_com_user(data_com)\n",
    "df_com_2 = sub_cat_in_com(data_com)\n",
    "df_com_3 = add_recipe_info(data_com)\n",
    "df_com_new = df_com_1.merge(df_com_2, on='recipe_id', how='left')\n",
    "df_com_new = df_com_new.merge(df_com_3, on='recipe_id', how='left')\n",
    "\n",
    "df_com_new = df_com_new[['recipe_id','recipe_name','tags','difficulty','preparation_time','ingredient','name','rating','sex','age','marriage_status','comment_time','calorie_value']]\n",
    "df_com_new = df_com_new.set_index([\"recipe_id\"])\n",
    "# df_com_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove none value in the whole data set \n",
    "df_com_no_none = df_com_new.mask(df_com_new.astype(object).eq('None')).dropna()\n",
    "df_com_new = df_com_no_none\n",
    "\n",
    "df_com_new = df_com_new.reset_index()\n",
    "df_com_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add calorie level and age group columns in the comment user information\n",
    "# df_com_new['calorie_level'] = df_com_new['calorie_value'].apply(calorie_level)\n",
    "# df_dum_car = pd.get_dummies(df_com_new['calorie_level'])\n",
    "# df_com_age = df_com_new\n",
    "# df_com_age[\"age_value\"] = df_com_age[\"age\"].str.extract(pat, flags=0, expand=True)\n",
    "# df_com_age['age_group'] = df_com_age['age_value'].apply(age_group)\n",
    "# df_com_car = df_com_new.join(df_dum_car, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add calorie level , preparation time and age group columns in the comment user information\n",
    "df_com_new[\"pretime_value\"] = df_com_new[\"preparation_time\"].str.extract(pat, flags=0, expand=True)\n",
    "df_com_new['pre_time_group'] = df_com_new['pretime_value'].apply(pre_time_group)\n",
    "\n",
    "df_com_new['calorie_level'] = df_com_new['calorie_value'].apply(calorie_level)\n",
    "df_dum_car = pd.get_dummies(df_com_new['calorie_level'])\n",
    "\n",
    "df_com_new[\"age_value\"] = df_com_new[\"age\"].str.extract(pat, flags=0, expand=True)\n",
    "df_com_new['age_group'] = df_com_new['age_value'].apply(age_group)\n",
    "df_com_car = df_com_new.join(df_dum_car, how='left')\n",
    "df_com_car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the target is calorie and features are the sex,age,marriage status of comment users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubgroup as ps\n",
    "data = df_com_car[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "target = ps.BinaryTarget('high_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_new['calorie_level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with imbalanced data\n",
    "\n",
    "\"weiblich\": \"männlich\" ratio is around 5:1, the data is quite imblanced with regard to the sex, we want to try to resample the dataset to help improve the data quality. We want to see if we can improve the quality of subgroup discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampling = df_com_new[['rating','sex','age_group','marriage_status','calorie_level','calorie_value']]\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# oversampling\n",
    "# Separate input features and target\n",
    "y = df_sampling['calorie_level']\n",
    "X = df_sampling.drop('calorie_level', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# concatenate our training data back together\n",
    "# X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "high_calorie = df_sampling[df_sampling['calorie_level'] == 'high_calorie']\n",
    "low_calorie = df_sampling[df_sampling['calorie_level'] == 'low_calorie']\n",
    "medium_calorie = df_sampling[df_sampling['calorie_level'] == 'medium_calorie']\n",
    "\n",
    "\n",
    "# upsample minority\n",
    "low_upsampled = resample(low_calorie,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(high_calorie), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "medium_upsampled = resample(medium_calorie,\n",
    "                          replace=True,\n",
    "                          n_samples=len(high_calorie), \n",
    "                          random_state=27) \n",
    "\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([high_calorie, low_upsampled])\n",
    "upsampled = pd.concat([upsampled, medium_upsampled])\n",
    "upsampled.reset_index(inplace = True)\n",
    "upsampled['calorie_level'].value_counts()\n",
    "# upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the dummyset into the upsampled dataset\n",
    "upsampled_dum = pd.get_dummies(upsampled['calorie_level'])\n",
    "upsampled_dum.reset_index(inplace = True)\n",
    "upsampled_dum = upsampled_dum.drop('index', axis=1)\n",
    "\n",
    "# prepare for the binary target columns\n",
    "# upsampled = upsampled.drop('index', axis=1)\n",
    "upsampled_new = upsampled.join(upsampled_dum, how='left')\n",
    "# upsampled_new = upsampled_new.drop('calorie_level',1)\n",
    "upsampled_new['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = upsampled_new[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "target = ps.BinaryTarget('low_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample majority\n",
    "high_downsampled = resample(high_calorie,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(low_calorie), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "medium_downsampled = resample(medium_calorie,\n",
    "                                replace = False,\n",
    "                                n_samples = len(low_calorie),\n",
    "                                random_state = 27)\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([high_downsampled, low_calorie])\n",
    "downsampled = pd.concat([downsampled, medium_downsampled])\n",
    "\n",
    "# add the dummyset into the downsampled dataset\n",
    "downsampled_dum = pd.get_dummies(downsampled['calorie_level'])\n",
    "# downsampled_dum.reset_index(inplace = True)\n",
    "# downsampled.reset_index(inplace = True)\n",
    "downsampled_new = downsampled.join(downsampled_dum, how='left')\n",
    "downsampled_new['calorie_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubgroup as ps\n",
    "data = downsampled_new[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "target = ps.BinaryTarget('low_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "- the subgroup young single female seems to prefer the \"low-calorie\" recipes\n",
    "- the subgroup married male seems to prefer the \"high-calorie\" recipes\n",
    "- the subgroup divorced middle-age male seems to prefer the \"medium-calorie\" recipes\n",
    "\n",
    "then our next step is to discover why differnt calorie-level is preferred by these particular groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing problems\n",
    "- Here we remove all rows which have nan, does it make sense to use other fill nan methods? \n",
    "- the quality of downsampling is better than upsampling, but there are only 1040 rows left, is it enough?\n",
    "- now the quality is around 0.01 is still low, how can we then to improve the quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the target is calorie level and features is the preparation time and difficulty of recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_car['pre_time_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = df_com_car[['high_calorie', 'low_calorie', 'medium_calorie','pre_time_group']]\n",
    "target = ps.BinaryTarget('low_calorie', True)\n",
    "searchspace = ps.create_selectors(data_s, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data_s, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- according the discovery in 3.0.1, we also use downsampling to handle the imbalance in preparation time group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampling_time = df_com_new[['rating','sex','age_group','marriage_status','calorie_level','pre_time_group','difficulty']]\n",
    "df_sampling_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate input features and target\n",
    "y = df_sampling_time['calorie_level']\n",
    "X = df_sampling_time.drop('calorie_level', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# concatenate our training data back together\n",
    "# X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "high_calorie_t = df_sampling_time[df_sampling_time['calorie_level'] == 'high_calorie']\n",
    "low_calorie_t = df_sampling_time[df_sampling_time['calorie_level'] == 'low_calorie']\n",
    "medium_calorie_t = df_sampling_time[df_sampling_time['calorie_level'] == 'medium_calorie']\n",
    "\n",
    "# downsample majority\n",
    "high_downsampled_t = resample(high_calorie_t,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(low_calorie), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "medium_downsampled_t = resample(medium_calorie_t,\n",
    "                                replace = False,\n",
    "                                n_samples = len(low_calorie),\n",
    "                                random_state = 27)\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled_t = pd.concat([high_downsampled_t, low_calorie_t])\n",
    "downsampled_t = pd.concat([downsampled_t, medium_downsampled_t])\n",
    "\n",
    "# add the dummyset into the downsampled dataset\n",
    "downsampled_dum_t = pd.get_dummies(downsampled_t['calorie_level'])\n",
    "# downsampled_dum.reset_index(inplace = True)\n",
    "# downsampled.reset_index(inplace = True)\n",
    "downsampled_new_t = downsampled_t.join(downsampled_dum_t, how='left')\n",
    "downsampled_new_t['calorie_level'].value_counts()\n",
    "downsampled_new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = downsampled_new_t[['high_calorie', 'low_calorie', 'medium_calorie','pre_time_group','difficulty']]\n",
    "# target = ps.BinaryTarget('low_calorie', True)\n",
    "# target = ps.BinaryTarget('high_calorie', True)\n",
    "target = ps.BinaryTarget('medium_calorie', True)\n",
    "searchspace = ps.create_selectors(data_s, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data_s, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- the target is calorie level/users and features is the preparation time of recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover why differnt calorie-level is preferred by these particular groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set target as colorie-level/ subgroup, still use subgroup discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_tag(df):\n",
    "    distinct_tags = []\n",
    "    for i in range(len(df)):\n",
    "        distinct_tags += df.iloc[i]['tags']\n",
    "    return list(set(distinct_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set target as subgroup, explore the tags of recipes, use multiclass-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_new = downsampled_new.reset_index()\n",
    "downsampled_new['tags'] = downsampled_new['tags'].astype(object)\n",
    "for i in range(len(downsampled_new)):\n",
    "    tags = downsampled_new.iloc[i]['tags']\n",
    "    tags = tags_preprocess(tags)\n",
    "    downsampled_new.at[i, 'tags'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for multiclass classifier\n",
    "df = downsampled_new[['tags', 'sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "distinct_tags = get_distinct_tag(df)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df['tags'] = df.tags.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df = pd.DataFrame(data = vectorizer.fit_transform(df.tags.tolist()), columns = distinct_tags)\n",
    "new_df[['sex', 'age_group', 'marriage_status', 'calorie_level']] = df[['sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "\n",
    "# set the target value by combining 'sex', 'age_group', 'marriage_status'\n",
    "new_df['combine'] = new_df[['sex', 'age_group', 'marriage_status']].values.tolist()\n",
    "new_df['combine'] = new_df['combine'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df.iloc[:, :-5]\n",
    "y = new_df.iloc[:, -1]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(clf_X):\n",
    "    num_document = len(clf_X)\n",
    "    idf_row = clf_X.sum(axis = 0, skipna = True) \n",
    "    idf_row = len(clf_X)/idf_row\n",
    "    return idf_row\n",
    "\n",
    "def computeTF(clf_X):\n",
    "    clf_X['sum'] = clf_X.sum(axis = 1, skipna = True)\n",
    "    clf_X = clf_X.div(clf_X['sum'], axis=0)\n",
    "    clf_X = clf_X.drop(['sum'],axis = 1)\n",
    "    return clf_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use td-idf to preprocess the data\n",
    "idf_row = computeIDF(clf_X)\n",
    "clf_X = computeTF(clf_X)\n",
    "clf_X = clf_X*idf_row\n",
    "clf_X = clf_X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifier after preprocessing with td-idf\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set target as subgroup, explore the ingredients of recipes, use multiclass-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for multi-class classifier\n",
    "distinct_ingredients, df_2 = ingredients_preprocess(downsampled_new)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df_2['ingredient'] = df_2.ingredient.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df_2 = pd.DataFrame(data = vectorizer.fit_transform(df_2.ingredient.tolist()), columns = distinct_ingredients)\n",
    "new_df_2[['sex', 'age_group', 'marriage_status', 'calorie_level']] = df_2[['sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "\n",
    "# set the target value by combining 'sex', 'age_group', 'marriage_status'\n",
    "new_df_2['combine'] = new_df_2[['sex', 'age_group', 'marriage_status']].values.tolist()\n",
    "new_df_2['combine'] = new_df_2['combine'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df_2.iloc[:, :-5]\n",
    "y = new_df_2.iloc[:, -1]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:20]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set target as calorie, explore the tags of recipes, use multiclass-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df.iloc[:, :-5]\n",
    "y = new_df.iloc[:, -2]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association rules among the recipe attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the association with \"ingredient\" and \"regions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['china', 'indien', 'deutschland','frankreich','grossbritannien','österreich','usaoderkanada','italien','spanien',\n",
    "             'portugal', 'japen','schweiz','türkei', 'thailand', 'russland', 'großbritannien & irland', 'vietnam', 'korea',\n",
    "            'australien', 'ägypten', 'marokko', 'niederlande']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_certain_countries = get_recipe_countries(countries, data)\n",
    "df_certain_countries.reset_index(inplace = True)\n",
    "df_certain_countries['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns other than \"ingredient\" and \"label\" column\n",
    "columns_drop = ['index', 'categorize', 'recipe_name', 'tags', 'avg_score', 'difficulty','rating_count', 'calorie', \n",
    "                'preparation_time','comment_user', 'recipe_url']\n",
    "df_certain_countries = df_certain_countries.drop(columns_drop, axis = 1)\n",
    "\n",
    "# preprocess the ingredient column\n",
    "distinct_ingredients = ingredients_preprocess(df_certain_countries)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df_certain_countries['ingredient'] = df_certain_countries.ingredient.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df_countries = pd.DataFrame(data = vectorizer.fit_transform(df_certain_countries.ingredient.tolist()), columns = distinct_ingredients)\n",
    "new_df_countries['label'] = df_certain_countries.label\n",
    "\n",
    "# dummy for the label column\n",
    "new_df_countries = pd.get_dummies(new_df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply \n",
    "new_df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = new_df_countries.iloc[:,]\n",
    "test = test.iloc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_label = list(new_df_countries.iloc[:,-19:].columns)\n",
    "#contry_label.remove('label_deutschland')\n",
    "\n",
    "# record the start time\n",
    "time1 = time.time()\n",
    "        \n",
    "# apply subgroup discovery \n",
    "target = ps.BinaryTarget ('label_usaoderkanada', True)\n",
    "searchspace = ps.create_selectors(test, ignore = country_label)\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=20, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "\n",
    "# record the end time\n",
    "time2 = time.time()\n",
    "time_diff = (time2-time1)/60\n",
    "print('it took ' + str(time_diff) + 'miniutes to execute the subgroup disc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
