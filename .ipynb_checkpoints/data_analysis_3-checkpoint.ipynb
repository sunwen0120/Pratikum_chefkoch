{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the library used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysubgroup as ps\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "import ast\n",
    "import string \n",
    "import imblearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_preprocess(tags):\n",
    "    \"\"\"\n",
    "    input: tag string\n",
    "    output: list of individual tags in the given tag string\n",
    "    function: preprocess a single tag string \n",
    "    \"\"\"\n",
    "    #tags = tags.replace(\"'\",\"\")\n",
    "    #tags = tags.replace(\" \",\"\")\n",
    "    #tags = tags.replace(\"[\",\"\")\n",
    "    #tags = tags.replace(\"]\",\"\")\n",
    "    tags = str(tags)\n",
    "    tags = tags.split(\" \")\n",
    "    tags = [x.lower() for x in tags]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredients_preprocess(df):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: list of distinct ingredient list\n",
    "    function: preprocess the ingredient columns and return a list of distinct ingredients\n",
    "    \"\"\"\n",
    "    distinct_ingredients = []\n",
    "    dataframe = df\n",
    "    \n",
    "    for i in range(len(dataframe)):\n",
    "        ingredients = dataframe.iloc[i]['ingredient']\n",
    "        \n",
    "        r = re.compile('[A-Z]{1}[a-zA-Z]+')\n",
    "        ingredients = str(ingredients)\n",
    "        #ingredients = ''.join(i for i in ingredients if not i.isdigit())\n",
    "        ingredients = ingredients.replace(\"'\",\"\")\n",
    "        #ingredients = ingredients.replace(\" \",\"\")\n",
    "        ingredients = ingredients.replace(\"[\",\"\")\n",
    "        ingredients = ingredients.replace(\"]\",\"\")\n",
    "        # remove text inside parentheses\n",
    "        ingredients = re.sub(r'\\([^())]*\\)',\"\", ingredients)\n",
    "        ingredients = ingredients.split(\" \")\n",
    "        ingredients = list(filter(r.match, ingredients))\n",
    "        ingredients = [x.lower() for x in ingredients]\n",
    "        distinct_ingredients += ingredients\n",
    "        dataframe.at[i, 'ingredient'] = ingredients\n",
    "        #dataframe.set_value(i, 'ingredient', ingredients)\n",
    "        \n",
    "    return [list(set(distinct_ingredients)),dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_countries(countries, data):\n",
    "    \"\"\"\n",
    "    input: list of countries, dataframe\n",
    "    output: selected dataframe whose recipes is from these countries\n",
    "    function: select the rows in dataframe whose \"tag\" value contain one country tag\n",
    "    \"\"\"\n",
    "    # add a new column class \n",
    "    drop_index = []\n",
    "    for i in range(len(data)):\n",
    "        tags = data.loc[i][\"tags\"]\n",
    "        tags = tags_preprocess(tags)\n",
    "        \n",
    "        country_same =[l for l in countries if l in tags]\n",
    "            \n",
    "        if len(country_same) == 1:\n",
    "            data.at[i, 'label'] = country_same[0]\n",
    "        if len(country_same) == 0:\n",
    "            drop_index.append(i)\n",
    "        if len(country_same) > 1:\n",
    "            drop_index.append(i)\n",
    "            #data.at[i, 'label'] = 'overlap'\n",
    "            \n",
    "    # drop the columns which has no season tags\n",
    "    data = data.drop(data.index[drop_index])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(arr):\n",
    "    \"\"\"\n",
    "    Helper function to convect an array of ingredients to a dictionary\n",
    "    \"\"\"\n",
    "    d={}\n",
    "    for a in arr:\n",
    "        d[a]=1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract comment user dataset from original dataset\n",
    "def extract_com_user(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with all the comment user information\n",
    "    function: spilt the dictionary of the column 'comment user' in original dataset\n",
    "    \"\"\"\n",
    "    df_com = pd.DataFrame()\n",
    "    for index, item in data['comment_user'].iteritems():\n",
    "        if (item != '[]'):\n",
    "            if (item != 'no comment'):\n",
    "                array = ast.literal_eval(item)\n",
    "                df_array = pd.DataFrame(array)\n",
    "                df_array['recipe_id'] = index\n",
    "                df_com = pd.concat([df_com,df_array])\n",
    "    return df_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_cat_in_com(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with multi colunms\n",
    "    function: add one subcategory of recipes to comment user dataset \n",
    "    \"\"\"    \n",
    "    punct = set(string.punctuation) \n",
    "    list_sub_cat = []\n",
    "    \n",
    "    df_com2 = pd.DataFrame()\n",
    "    for index, item in data['calorie_value'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_sub = list(item)\n",
    "            list_sub = ''.join(x for x in list_sub if x not in punct)\n",
    "            list_sub_cat.append(list_sub)\n",
    "    df_sub_cat = pd.DataFrame(list_sub_cat)\n",
    "    df_sub_cat['calorie_value'] = df_sub_cat\n",
    "    \n",
    "    df_sub_cat['recipe_id'] = data['calorie_value'].index     \n",
    "    df_com2 = pd.concat([df_com2,df_sub_cat])\n",
    "    return df_com2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recipe_info(data):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    output: a new dataframe with multi colunms\n",
    "    function: add one subcategory of recipes to comment user dataset \n",
    "    \"\"\"    \n",
    "    punct = set(string.punctuation) \n",
    "    # add recipe name in comment users data\n",
    "    list_recipe_name = []\n",
    "    df_recipe = pd.DataFrame()\n",
    "    for index, item in data['recipe_name'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_name = list(item)\n",
    "            list_name = ''.join(x for x in list_name if x not in punct)\n",
    "            list_recipe_name.append(list_name)\n",
    "\n",
    "    df_name = pd.DataFrame(list_recipe_name)\n",
    "    df_name['recipe_id'] = data['recipe_name'].index \n",
    "    df_name = df_name.set_index([\"recipe_id\"])\n",
    "    df_name\n",
    "    df_recipe['recipe_name'] = df_name[0]\n",
    "\n",
    "    # add recipe difficulty in comment users data\n",
    "    list_recipe_diff = []\n",
    "    df_recipe_diff = pd.DataFrame()\n",
    "    for index, item in data['difficulty'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_diff = list(item)      \n",
    "            list_diff = ''.join(x for x in list_diff if x not in punct)   \n",
    "            list_recipe_diff.append(list_diff)\n",
    "\n",
    "    df_diff = pd.DataFrame(list_recipe_diff)\n",
    "    df_diff['recipe_id'] = data['difficulty'].index \n",
    "    df_diff = df_diff.set_index([\"recipe_id\"])\n",
    "    df_recipe['difficulty'] = df_diff[0]\n",
    "\n",
    "    # add recipe preparation_time in comment users data\n",
    "    list_recipe_pre = []\n",
    "    df_recipe_pre = pd.DataFrame()\n",
    "    for index, item in data['preparation_time'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_pre = list(item)\n",
    "\n",
    "            list_pre = ''.join(x for x in list_pre if x not in punct)\n",
    "\n",
    "            list_recipe_pre.append(list_pre)\n",
    "\n",
    "    df_pre = pd.DataFrame(list_recipe_pre)\n",
    "    df_pre['recipe_id'] = data['preparation_time'].index \n",
    "    df_pre = df_pre.set_index([\"recipe_id\"])\n",
    "    df_recipe['preparation_time'] = df_pre[0]\n",
    "\n",
    "    # add recipe tags in comment users data\n",
    "    list_recipe_tags = []\n",
    "    df_recipe_tags = pd.DataFrame()\n",
    "    for index, item in data['tags'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_tags = list(item)      \n",
    "            list_tags = ''.join(x for x in list_tags if x not in punct)   \n",
    "            list_recipe_tags.append(list_tags)\n",
    "\n",
    "\n",
    "    df_tags = pd.DataFrame(list_recipe_tags)\n",
    "    df_tags['recipe_id'] = data['tags'].index \n",
    "    df_tags = df_tags.set_index([\"recipe_id\"])\n",
    "    df_recipe['tags'] = df_tags[0]\n",
    "\n",
    "    # add recipe ingredient in comment users data\n",
    "    list_recipe_ingredient = []\n",
    "    df_recipe_ingredient = pd.DataFrame()\n",
    "    for index, item in data['ingredient'].iteritems(): \n",
    "        if (item != None):\n",
    "            list_ingredient = list(item) \n",
    "            list_ingredient = ''.join(x for x in list_ingredient if x not in punct)\n",
    "            list_recipe_ingredient.append(list_ingredient)\n",
    "\n",
    "    df_recipe_ingredient = pd.DataFrame(list_recipe_ingredient)\n",
    "    df_recipe_ingredient['recipe_id'] = data['ingredient'].index \n",
    "    df_recipe_ingredient = df_recipe_ingredient.set_index([\"recipe_id\"])\n",
    "    df_recipe['ingredient'] = df_recipe_ingredient[0]\n",
    "    return df_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_group(age):   \n",
    "    \"\"\"\n",
    "    input: age value\n",
    "    output: group description\n",
    "    function: divide age value uinto 5 groups \n",
    "    \"\"\"   \n",
    "    bucket = None\n",
    "    age = int(age)    \n",
    "    if age <= 30:\n",
    "        bucket = '<30 Jahre'     \n",
    "    if age in range(30, 46):\n",
    "        bucket = '30-45 Jahre'\n",
    "    if age in range(45, 61):\n",
    "        bucket = '45-60 Jahre'\n",
    "    if age >= 61:\n",
    "        bucket = '60+ Jahre'\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calorie_level(calorie):   \n",
    "    \"\"\"\n",
    "    input: calorie value\n",
    "    output: group description\n",
    "    function: divide calorie value into 3 groups \n",
    "    \"\"\"   \n",
    "    bucket = None\n",
    "    calorie = int(calorie)    \n",
    "    if calorie <= 300:\n",
    "        bucket = 'low_calorie'    \n",
    "    if calorie in range(300, 500):\n",
    "        bucket = 'medium_calorie'        \n",
    "    if calorie >= 500:\n",
    "        bucket = 'high_calorie'      \n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_None(data, name):\n",
    "    \"\"\"\n",
    "    Helper function to remove None value in one column\n",
    "    \"\"\" \n",
    "    y = data[data[name] == 'None']\n",
    "    index_n = y.index.tolist()\n",
    "    data = data.drop(index = index_n)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(data, calorie_level):\n",
    "    df_sub_group[country] = df_dum_car[calorie_level]\n",
    "    return df_sub_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_time_group(pre_time):   \n",
    "    pre_time = int(pre_time)    \n",
    "    if pre_time <= 20:\n",
    "        bucket = '<20 Min'    \n",
    "    if pre_time in range(20, 31):\n",
    "        bucket = '20-30 Min'                \n",
    "    if pre_time >= 31:\n",
    "        bucket = '30+ Min'\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgroup Discovery\n",
    "\n",
    "In this section we will use subgroup discovery to explore the association rules between attributes\n",
    "\n",
    "- why we choose subgroup discovery?\n",
    "\n",
    "because we find out that subgroup discovery is quite powerful compared to other data mining techniques. As long as we set differnt target with different search space, we can use use subgroup discovery technique to dig many interesting patterns that we want to explore from the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'all_data.csv' does not exist: b'all_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f10b150fe1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# data = pd.read_csv(\"/Users/xujingjing/Desktop/2020 Sommer/praktikum/Data/all_data.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'all_data.csv' does not exist: b'all_data.csv'"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"all_data.csv\")\n",
    "# data = pd.read_csv(\"/Users/xujingjing/Desktop/2020 Sommer/praktikum/Data/all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the recipe id from recipe urls\n",
    "list_cat_no = []\n",
    "i = 0\n",
    "for item in data['recipe_url']:\n",
    "    list_cat_no.append(item.split('/')[4])\n",
    "    \n",
    "# add one column \"recipe_id\" into the dataset and set it as the index of dataset\n",
    "data['recipe_id'] = list_cat_no\n",
    "data = data.set_index([\"recipe_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules between comment user information and recipe attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### associations with \"calorie\" and comment user information\n",
    "\n",
    "since the job has more than half non values, so here we only explore the the association between  marriage_status, gender, and age with recipe calorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the numerical value string in the column calorie \n",
    "pat = r\"([-+]?\\d*\\.\\d+|\\d+)\"\n",
    "data[\"calorie_value\"] = data[\"calorie\"].str.extract(pat, flags=0, expand=True)\n",
    "\n",
    "# drop all rows with nan value in both columns comment_user and calorie_value\n",
    "data_com = data.dropna(subset=[\"comment_user\",'calorie_value'])\n",
    "len(data_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the corresponding recipe data for each comment_user\n",
    "df_com_1 = extract_com_user(data_com)\n",
    "df_com_2 = sub_cat_in_com(data_com)\n",
    "df_com_3 = add_recipe_info(data_com)\n",
    "df_com_new = df_com_1.merge(df_com_2, on='recipe_id', how='left')\n",
    "df_com_new = df_com_new.merge(df_com_3, on='recipe_id', how='left')\n",
    "\n",
    "df_com_new = df_com_new[['recipe_id','recipe_name','tags','difficulty','preparation_time','ingredient','name','rating','sex','age','marriage_status','comment_time','calorie_value']]\n",
    "df_com_new = df_com_new.set_index([\"recipe_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the data in Getränk dataset\n",
    "df_com_new.reset_index(inplace = True)\n",
    "print(len(df_com_new))\n",
    "\n",
    "\n",
    "for i in range(len(df_com_new)): \n",
    "    index_g = []\n",
    "    tags = df_com_new.loc[i]['tags']\n",
    "    tags = tags_preprocess(tags)\n",
    "\n",
    "    if \"getränk\" in tags:\n",
    "        index_g.append(i)\n",
    "#         print(index_g)\n",
    "        df_com_new = df_com_new.drop(index = index_g)\n",
    "print(len(df_com_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove none value in the whole data set \n",
    "df_com_no_none = df_com_new.mask(df_com_new.astype(object).eq('None')).dropna()\n",
    "df_com_no_none = df_com_no_none.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and add calorie level , preparation time and age group columns in the comment user information\n",
    "df_com_no_none[\"pretime_value\"] = df_com_no_none[\"preparation_time\"].str.extract(pat, flags=0, expand=True)\n",
    "df_com_no_none['pre_time_group'] = df_com_no_none['pretime_value'].apply(pre_time_group)\n",
    "\n",
    "df_com_no_none['calorie_level'] = df_com_no_none['calorie_value'].apply(calorie_level)\n",
    "df_dum_calorie = pd.get_dummies(df_com_no_none['calorie_level'])\n",
    "\n",
    "df_com_no_none[\"age_value\"] = df_com_no_none[\"age\"].str.extract(pat, flags=0, expand=True)\n",
    "df_com_no_none['age_group'] = df_com_no_none['age_value'].apply(age_group)\n",
    "\n",
    "df_com_dum_calorie = df_com_no_none.join(df_dum_calorie, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_dum_calorie['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_dum_calorie['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_dum_calorie['calorie_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_dum_calorie['marriage_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup discovery before up/down sampling\n",
    "\n",
    "the target is calorie and features are the sex,age,marriage status of comment users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_com_dum_calorie[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "print(\"the length of dataset:\", len(data))\n",
    "target = ps.BinaryTarget('low_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with imbalanced data\n",
    "\n",
    "\"high_calorie\": \"medium_calorie\": \"low_calorie\" ratio is around 2.5:1:1, the data is quite imblanced with regard to the calorie level, we want to try to resample the dataset to help improve the data quality. We want to see if we can improve the quality of subgroup discovery\n",
    "\n",
    "- upsampling: upsample \"medium_calorie\" and \"low_calorie\" samples to make them equal to the number of \"high_calorie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampling = df_com_dum_calorie[['rating','sex','age_group','marriage_status','calorie_level','pre_time_group','difficulty','tags','ingredient']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "# Separate input features and target\n",
    "y = df_sampling['calorie_level']\n",
    "X = df_sampling.drop('calorie_level', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# concatenate our training data back together\n",
    "# X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "high_calorie = df_sampling[df_sampling['calorie_level'] == 'high_calorie']\n",
    "low_calorie = df_sampling[df_sampling['calorie_level'] == 'low_calorie']\n",
    "medium_calorie = df_sampling[df_sampling['calorie_level'] == 'medium_calorie']\n",
    "\n",
    "\n",
    "# upsample minority\n",
    "low_upsampled = resample(low_calorie,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(high_calorie), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "medium_upsampled = resample(medium_calorie,\n",
    "                          replace=True,\n",
    "                          n_samples=len(high_calorie), \n",
    "                          random_state=27) \n",
    "\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([high_calorie, low_upsampled])\n",
    "upsampled = pd.concat([upsampled, medium_upsampled])\n",
    "upsampled.reset_index(inplace = True)\n",
    "print(\"After upsampling:\")\n",
    "upsampled['calorie_level'].value_counts()\n",
    "# upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the dummyset into the upsampled dataset\n",
    "upsampled_dum = pd.get_dummies(upsampled['calorie_level'])\n",
    "upsampled_dum.reset_index(inplace = True)\n",
    "upsampled_dum = upsampled_dum.drop('index', axis=1)\n",
    "\n",
    "# prepare for the binary target columns\n",
    "# upsampled = upsampled.drop('index', axis=1)\n",
    "upsampled_new = upsampled.join(upsampled_dum, how='left')\n",
    "# upsampled_new = upsampled_new.drop('calorie_level',1)\n",
    "# upsampled_new['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = upsampled_new[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "target = ps.BinaryTarget('medium_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample majority\n",
    "high_downsampled = resample(high_calorie,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(low_calorie), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "medium_downsampled = resample(medium_calorie,\n",
    "                                replace = False,\n",
    "                                n_samples = len(low_calorie),\n",
    "                                random_state = 27)\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([high_downsampled, low_calorie])\n",
    "downsampled = pd.concat([downsampled, medium_downsampled])\n",
    "\n",
    "# add the dummyset into the downsampled dataset\n",
    "downsampled_dum = pd.get_dummies(downsampled['calorie_level'])\n",
    "# downsampled_dum.reset_index(inplace = True)\n",
    "# downsampled.reset_index(inplace = True)\n",
    "downsampled_new = downsampled.join(downsampled_dum, how='left')\n",
    "print(\"After downsampling:\")\n",
    "downsampled_new['calorie_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = downsampled_new[['sex', 'age_group', 'high_calorie', 'low_calorie', 'medium_calorie','marriage_status']]\n",
    "target = ps.BinaryTarget('medium_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover why differnt calorie-level is preferred by these particular groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_dum_calorie['pre_time_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = downsampled_new[['high_calorie', 'low_calorie', 'medium_calorie','pre_time_group','difficulty']]\n",
    "target = ps.BinaryTarget('medium_calorie', True)\n",
    "searchspace = ps.create_selectors(data, ignore=['high_calorie', 'low_calorie', 'medium_calorie'])\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=2, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "pd.set_option('max_colwidth',100)\n",
    "print(result.to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = downsampled_new[['sex','age_group','marriage_status','pre_time_group','difficulty','calorie_level']]\n",
    "\n",
    "#  set the target value by combining 'sex', 'age_group', 'marriage_status'\n",
    "new_data['combine'] = new_data[['sex', 'age_group', 'marriage_status']].values.tolist()\n",
    "new_data['combine'] = new_data['combine'].astype(str)\n",
    "\n",
    "# one hot embedding of target value\n",
    "combine_dum = pd.get_dummies(new_data['combine'])\n",
    "new_data = new_data.join(combine_dum, how='left')\n",
    "# combine_dum\n",
    "# new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover the relationship bewteen all the comment user groups and the preparation time and difficulty of recipes\n",
    "\n",
    "data = new_data[['pre_time_group','difficulty']]\n",
    "i =0\n",
    "for col in combine_dum:\n",
    "    column_name = col\n",
    "    column_name = str(column_name)\n",
    "    data[column_name] = new_data[column_name]\n",
    "\n",
    "    target = ps.BinaryTarget(column_name, True)\n",
    "    searchspace = ps.create_selectors(data, ignore=[column_name])\n",
    "    task = ps.SubgroupDiscoveryTask (\n",
    "        data, \n",
    "        target, \n",
    "        searchspace, \n",
    "        result_set_size=5, \n",
    "        depth=2, \n",
    "        qf=ps.WRAccQF())\n",
    "    result = ps.BeamSearch().execute(task)\n",
    "    pd.set_option('max_colwidth',100)\n",
    "    result = result.to_dataframe()\n",
    "    result = result.rename(columns={'description':column_name})\n",
    "    data = data.drop(column_name,1)\n",
    "    i = i+1\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(\"number of target combination is:\", i)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_tag(df):\n",
    "    distinct_tags = []\n",
    "    for i in range(len(df)):\n",
    "        distinct_tags += df.iloc[i]['tags']\n",
    "    return list(set(distinct_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_new = downsampled_new.reset_index()\n",
    "downsampled_new['tags'] = downsampled_new['tags'].astype(object)\n",
    "for i in range(len(downsampled_new)):\n",
    "    tags = downsampled_new.iloc[i]['tags']\n",
    "    tags = tags_preprocess(tags)\n",
    "    downsampled_new.at[i, 'tags'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for multiclass classifier\n",
    "df = downsampled_new[['tags', 'sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "distinct_tags = get_distinct_tag(df)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df['tags'] = df.tags.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df = pd.DataFrame(data = vectorizer.fit_transform(df.tags.tolist()), columns = distinct_tags)\n",
    "new_df[['sex', 'age_group', 'marriage_status', 'calorie_level']] = df[['sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "\n",
    "# set the target value by combining 'sex', 'age_group', 'marriage_status'\n",
    "new_df['combine'] = new_df[['sex', 'age_group', 'marriage_status']].values.tolist()\n",
    "new_df['combine'] = new_df['combine'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df.iloc[:, :-5]\n",
    "y = new_df.iloc[:, -1]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(clf_X):\n",
    "    num_document = len(clf_X)\n",
    "    idf_row = clf_X.sum(axis = 0, skipna = True) \n",
    "    idf_row = len(clf_X)/idf_row\n",
    "    return idf_row\n",
    "\n",
    "def computeTF(clf_X):\n",
    "    clf_X['sum'] = clf_X.sum(axis = 1, skipna = True)\n",
    "    clf_X = clf_X.div(clf_X['sum'], axis=0)\n",
    "    clf_X = clf_X.drop(['sum'],axis = 1)\n",
    "    return clf_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use td-idf to preprocess the data\n",
    "idf_row = computeIDF(clf_X)\n",
    "clf_X = computeTF(clf_X)\n",
    "clf_X = clf_X*idf_row\n",
    "clf_X = clf_X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifier after preprocessing with td-idf\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 =['weiblich', '30-45 Jahre', 'Verheiratet']\n",
    "a_1 = str(a_1)\n",
    "a_2 = ['weiblich', '30-45 Jahre', 'Vergeben']\n",
    "a_2 = str(a_2)\n",
    "df[[a_1,a_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = ['männlich', '45-60 Jahre', 'Verheiratet']\n",
    "b_1 = str(b_1)\n",
    "columns = list(df.columns)\n",
    "cols_b = [l for l in columns if \"Verheiratet\" in l and \"60+ Jahre\" in l]\n",
    "cols_b.append(b_1)\n",
    "df[cols_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_c = [l for l in columns if \"weiblich\" in l and \"60+ Jahre\" in l]\n",
    "cols_c += [l for l in columns if \"weiblich\" in l and \"Verwitwet\" in l]\n",
    "df[cols_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for multi-class classifier\n",
    "distinct_ingredients, df_2 = ingredients_preprocess(downsampled_new)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df_2['ingredient'] = df_2.ingredient.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df_2 = pd.DataFrame(data = vectorizer.fit_transform(df_2.ingredient.tolist()), columns = distinct_ingredients)\n",
    "new_df_2[['sex', 'age_group', 'marriage_status', 'calorie_level']] = df_2[['sex', 'age_group', 'marriage_status', 'calorie_level']]\n",
    "\n",
    "# set the target value by combining 'sex', 'age_group', 'marriage_status'\n",
    "new_df_2['combine'] = new_df_2[['sex', 'age_group', 'marriage_status']].values.tolist()\n",
    "new_df_2['combine'] = new_df_2['combine'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df_2.iloc[:, :-5]\n",
    "y = new_df_2.iloc[:, -1]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 =['weiblich', '30-45 Jahre', 'Verheiratet']\n",
    "a_1 = str(a_1)\n",
    "a_2 = ['weiblich', '30-45 Jahre', 'Vergeben']\n",
    "a_2 = str(a_2)\n",
    "df[[a_1,a_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = ['männlich', '45-60 Jahre', 'Verheiratet']\n",
    "b_1 = str(b_1)\n",
    "columns = list(df.columns)\n",
    "cols_b = [l for l in columns if \"Verheiratet\" in l and \"60+ Jahre\" in l]\n",
    "cols_b.append(b_1)\n",
    "df[cols_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_c = [l for l in columns if \"weiblich\" in l and \"60+ Jahre\" in l]\n",
    "cols_c += [l for l in columns if \"weiblich\" in l and \"Verwitwet\" in l]\n",
    "df[cols_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classifer\n",
    "clf_X = new_df.iloc[:, :-5]\n",
    "y = new_df.iloc[:, -2]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(clf_X, y)\n",
    "y_pred = clf.predict(clf_X)\n",
    "\n",
    "# accuracy \n",
    "print(\"Accuracy:\",clf.score(clf_X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(clf.classes_)):\n",
    "    if clf.classes_[i] != 'overlap':\n",
    "        df[clf.classes_[i]] = [vectorizer.feature_names_[ing_id] for ing_id in np.argsort(-clf.coef_[i])[:10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association rules among the recipe attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['china', 'indien', 'deutschland','frankreich','grossbritannien','österreich','usaoderkanada','italien','spanien',\n",
    "             'portugal', 'japen','schweiz','türkei', 'thailand', 'russland', 'großbritannien & irland', 'vietnam', 'korea',\n",
    "            'australien', 'ägypten', 'marokko', 'niederlande']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_certain_countries = get_recipe_countries(countries, data)\n",
    "df_certain_countries.reset_index(inplace = True)\n",
    "df_certain_countries['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns other than \"ingredient\" and \"label\" column\n",
    "columns_drop = ['index', 'categorize', 'recipe_name', 'tags', 'avg_score', 'difficulty','rating_count', 'calorie', \n",
    "                'preparation_time','comment_user', 'recipe_url']\n",
    "df_certain_countries = df_certain_countries.drop(columns_drop, axis = 1)\n",
    "\n",
    "# preprocess the ingredient column\n",
    "distinct_ingredients = ingredients_preprocess(df_certain_countries)\n",
    "\n",
    "# One hot encoding of the ingredients\n",
    "df_certain_countries['ingredient'] = df_certain_countries.ingredient.apply(convert_to_dict)\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "new_df_countries = pd.DataFrame(data = vectorizer.fit_transform(df_certain_countries.ingredient.tolist()), columns = distinct_ingredients)\n",
    "new_df_countries['label'] = df_certain_countries.label\n",
    "\n",
    "# dummy for the label column\n",
    "new_df_countries = pd.get_dummies(new_df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply \n",
    "new_df_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = new_df_countries.iloc[:,]\n",
    "test = test.iloc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_label = list(new_df_countries.iloc[:,-19:].columns)\n",
    "#contry_label.remove('label_deutschland')\n",
    "\n",
    "# record the start time\n",
    "time1 = time.time()\n",
    "        \n",
    "# apply subgroup discovery \n",
    "target = ps.BinaryTarget ('label_usaoderkanada', True)\n",
    "searchspace = ps.create_selectors(test, ignore = country_label)\n",
    "task = ps.SubgroupDiscoveryTask (\n",
    "    data, \n",
    "    target, \n",
    "    searchspace, \n",
    "    result_set_size=5, \n",
    "    depth=20, \n",
    "    qf=ps.WRAccQF())\n",
    "result = ps.BeamSearch().execute(task)\n",
    "\n",
    "# record the end time\n",
    "time2 = time.time()\n",
    "time_diff = (time2-time1)/60\n",
    "print('it took ' + str(time_diff) + 'miniutes to execute the subgroup disc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
